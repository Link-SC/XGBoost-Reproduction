import numpy as np
from sklearn.tree import DecisionTreeRegressor
from sklearn.base import BaseEstimator, RegressorMixin

class WQS_XGB(BaseEstimator, RegressorMixin):
    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3, 
                 reg_lambda=1.0, gamma=0, min_child_weight=1, 
                 subsample=1.0, colsample_bytree=1.0, num_quantiles=10):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.max_depth = max_depth
        self.reg_lambda = reg_lambda
        self.gamma = gamma
        self.min_child_weight = min_child_weight
        self.subsample = subsample
        self.colsample_bytree = colsample_bytree
        self.num_quantiles = num_quantiles
        self.trees = []
        self.base_score = None
        self.feature_importances_ = None

    def _weighted_quantile_sketch(self, values, weights, quantiles):
        sorted_idx = np.argsort(values)
        sorted_values = values[sorted_idx]
        sorted_weights = weights[sorted_idx]
        
        cum_weights = np.cumsum(sorted_weights)
        total_weight = cum_weights[-1]
        
        targets = total_weight * np.array(quantiles)
        splits = []
        current_idx = 0
        
        for target in targets:
            while current_idx < len(cum_weights) and cum_weights[current_idx] < target:
                current_idx += 1
            if current_idx < len(sorted_values):
                splits.append(sorted_values[current_idx])
        return np.unique(splits)

    def _calculate_gain(self, G, H, G_left, H_left, G_right, H_right):
        def _gain(G, H):
            return G**2 / (H + self.reg_lambda)
        return (_gain(G_left, H_left) + _gain(G_right, H_right) - _gain(G, H)) / 2 - self.gamma

    def _find_best_split(self, X, grad, hess, feature_idx):
        values = X[:, feature_idx]
        weights = hess
        
        quantiles = np.linspace(0, 1, self.num_quantiles + 2)[1:-1]
        candidate_splits = self._weighted_quantile_sketch(values, weights, quantiles)
        
        if len(candidate_splits) == 0:return None, None
        
        best_gain = -np.inf
        best_split = None
        
        for split in candidate_splits:
            left_mask = values <= split
            G_left = grad[left_mask].sum()
            H_left = hess[left_mask].sum()
            G_right = grad.sum() - G_left
            H_right = hess.sum() - H_left
            
            if H_left < self.min_child_weight or H_right < self.min_child_weight:
                continue 
            current_gain = self._calculate_gain(grad.sum(), hess.sum(), G_left, H_left, G_right, H_right)
            if current_gain > best_gain:
                best_gain = current_gain
                best_split = split
        return best_split, best_gain

    def _build_tree(self, X, grad, hess, depth=0):
        if depth >= self.max_depth or hess.sum() < 2 * self.min_child_weight:
            leaf_value = -grad.sum() / (hess.sum() + self.reg_lambda)
            return {'leaf': True, 'value': leaf_value}
        
        best_gain = -np.inf
        best_feature = None
        best_split = None

        n_features = X.shape[1]
        sampled_features = np.random.choice(
            n_features, 
            size=int(n_features * self.colsample_bytree), 
            replace=False
        )

        for feature_idx in sampled_features:
            split, gain = self._find_best_split(X, grad, hess, feature_idx)
            if gain is not None and gain > best_gain:
                best_gain = gain
                best_feature = feature_idx
                best_split = split

        if best_gain <= 0:
            leaf_value = -grad.sum() / (hess.sum() + self.reg_lambda)
            return {'leaf': True, 'value': leaf_value}

        left_mask = X[:, best_feature] <= best_split
        right_mask = ~left_mask
        
        left_child = self._build_tree(X[left_mask], grad[left_mask], hess[left_mask], depth+1)
        right_child = self._build_tree(X[right_mask], grad[right_mask], hess[right_mask], depth+1)
        
        return {
            'leaf': False,
            'feature': best_feature,
            'split': best_split,
            'left': left_child,
            'right': right_child
        }

    def _predict_tree(self, tree, x):
        if tree['leaf']:
            return tree['value']
        if x[tree['feature']] <= tree['split']:
            return self._predict_tree(tree['left'], x)
        else:
            return self._predict_tree(tree['right'], x)

    def fit(self, X, y):
        self.base_score = np.mean(y)
        self.trees = []
        pred = np.full_like(y, self.base_score)
        n_samples = X.shape[0]
        
        self.feature_importances_ = np.zeros(X.shape[1])
        
        for _ in range(self.n_estimators):
            if self.subsample < 1.0:
                sample_idx = np.random.choice(
                    n_samples, 
                    size=int(n_samples * self.subsample), 
                    replace=False
                )
                X_sample = X[sample_idx]
                y_sample = y[sample_idx]
                pred_sample = pred[sample_idx]
            else:
                X_sample = X
                y_sample = y
                pred_sample = pred

            grad = 2 * (pred_sample - y_sample)
            hess = 2 * np.ones_like(y_sample)

            tree = self._build_tree(X_sample, grad, hess)
            self.trees.append(tree)

            pred += self.learning_rate * np.array([self._predict_tree(tree, x) for x in X])

            self._update_feature_importance(tree)
            
        return self

    def _update_feature_importance(self, tree):
        if not tree['leaf']:
            self.feature_importances_[tree['feature']] += 1
            self._update_feature_importance(tree['left'])
            self._update_feature_importance(tree['right'])

    def predict(self, X):
        if self.base_score is None:
            raise ValueError("Model not fitted yet")
        pred = np.full(X.shape[0], self.base_score)
        for tree in self.trees:
            pred += self.learning_rate * np.array([self._predict_tree(tree, x) for x in X])
        return pred
